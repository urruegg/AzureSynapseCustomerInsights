{
	"name": "5_prepare_predictionsdata_for_ci",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "cb093d9a-24b6-43bf-a7ec-899c6c759da7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f39bf952-6a0c-4fac-bf30-0bf640da3752/resourceGroups/ursrueggcom/providers/Microsoft.Synapse/workspaces/ursrueggcomworkspace/bigDataPools/spark1",
				"name": "spark1",
				"type": "Spark",
				"endpoint": "https://ursrueggcomworkspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Copyright (c) Microsoft Corporation.\n",
					"\n",
					"Licensed under the MIT License."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Library Imports"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from notebookutils import mssparkutils\n",
					"import json"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read in Data from Azure Data Lake"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"data_lake_account_name = 'https://ursrueggcomdatalake.dfs.core.windows.net' # Synapse Workspace ADLS\n",
					"file_system_name = 'users/sourcedata'\n",
					"synapse_workspace_name = 'ursrueggcomworkspace'"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"try:\n",
					"    spark.sql(\"CREATE DATABASE c360_data\")\n",
					"except:\n",
					"    print(\"Database already exists\")\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"id_columns = ['cid','pid','ptid','uid','utid','lid','sid','paymentid','PostCode']\n",
					"date_columns = ['DateOfBirth']"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"def get_ci_datedefinitions():\n",
					"    definitions = []\n",
					"    d = {\n",
					"            \"traitName\": \"is.formatted\",\n",
					"            \"extendsTrait\": \"is\",\n",
					"            \"explanation\": \"a root for traits that descibe how data is formatted\"\n",
					"        }\n",
					"    definitions.append(d)\n",
					"    d = {\n",
					"            \"traitName\": \"is.formatted.dateTime\",\n",
					"            \"extendsTrait\": \"is.formatted\",\n",
					"            \"explanation\": \"DateTime data formatted as a string in ISO 8601 format\",\n",
					"            \"hasParameters\": [{\n",
					"                \"name\": \"format\",\n",
					"                \"dataType\": \"stringFormat\",\n",
					"                \"defaultValue\": \"MM/DD/YYYY hh:mm\"\n",
					"            }]\n",
					"        }\n",
					"    definitions.append(d)\n",
					"    d = {\n",
					"            \"traitName\": \"is.formatted.date\",\n",
					"            \"extendsTrait\": \"is.formatted\",\n",
					"            \"explanation\": \"Date data formatted as a string in ISO 8601 format\",\n",
					"            \"hasParameters\": [{\n",
					"                \"name\": \"format\",\n",
					"                \"dataType\": \"stringFormat\",\n",
					"                \"defaultValue\": \"MM/DD/YYYY\"\n",
					"            }]\n",
					"        }\n",
					"    definitions.append(d)\n",
					"    d = {\n",
					"            \"traitName\": \"is.formatted.time\",\n",
					"            \"extendsTrait\": \"is.formatted\",\n",
					"            \"explanation\": \"Time data formatted as a string in ISO 8601 format\",\n",
					"            \"hasParameters\": [{\n",
					"                \"name\": \"format\",\n",
					"                \"dataType\": \"stringFormat\",\n",
					"                \"defaultValue\": \"hh:mm:ss\"\n",
					"            }]\n",
					"        }\n",
					"    definitions.append(d)\n",
					"    return(definitions)\n",
					""
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"#create manifest.json\n",
					"\n",
					"filenames = ['lease_renewal_prediction']\n",
					"\n",
					"filebasepath = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/synapse/workspaces/{synapse_workspace_name}/warehouse/c360_data.db/'\n",
					"dfspath = 'https://'+ data_lake_account_name +'.dfs.core.windows.net/'+ file_system_name + '/synapse/workspaces/' + synapse_workspace_name + '/warehouse/c360_data.db/'\n",
					"id_columns = ['cid','pid','ptid','uid','utid','lid','sid','paymentid','CustomerId']\n",
					"curreny_columns = ['amount','RenewalPredictionScore']\n",
					"\n",
					"entities = []\n",
					"\n",
					"for filename in filenames:\n",
					"    df = spark.read.load(filebasepath + filename.lower(), format='parquet', header=True,inferSchema=True)\n",
					"    attributes = []\n",
					"    partitions = []\n",
					"    contents = []\n",
					"    for e in df.schema:\n",
					"        if e.name in id_columns:\n",
					"            edatatype = 'String'\n",
					"        elif e.name in curreny_columns:\n",
					"           edatatype = 'Double'\n",
					"        elif str(e.dataType) == 'StringType':\n",
					"            edatatype = 'String'\n",
					"        elif str(e.dataType) == 'TimestampType':\n",
					"            edatatype = 'DateTime'\n",
					"        elif str(e.dataType) == 'DateType':\n",
					"            edatatype = 'Date'\n",
					"        elif str(e.dataType) == 'IntegerType':\n",
					"            edatatype = 'Int32' #'Integer'\n",
					"        else:\n",
					"            edatatype = 'String'\n",
					"        \n",
					"\n",
					"        if edatatype != 'DateTime':\n",
					"          attr ={\n",
					"              \"name\": e.name,\n",
					"              \"dataFormat\": edatatype,\n",
					"          }\n",
					"        else:\n",
					"          attr ={\n",
					"              \"name\": e.name,\n",
					"              \"appliedTraits\": [\n",
					"                  \"is.formatted.dateTime\"\n",
					"                ],\n",
					"              \"dataFormat\": edatatype\n",
					"          }\n",
					"\n",
					"        attributes.append(attr)\n",
					"        content = {\n",
					"            \"type\" : \"attributeDefinition\",\n",
					"            \"name\" : e.name,\n",
					"            \"parent\" : filename + \"/attributeContext/\" + filename,\n",
					"            \"definition\" : \"resolvedFrom/\" + filename + \"/hasAttributes/\" + e.name,\n",
					"            \"contents\" : [\n",
					"              filename + \"/hasAttributes/\" + e.name\n",
					"            ]\n",
					"          }\n",
					"        contents.append(content)\n",
					"        #break\n",
					"\n",
					"    ent_imports = []\n",
					"    d = {\n",
					"        \"corpusPath\": '/' + filename + \".cdm.json\",\n",
					"        \"moniker\": \"resolvedFrom\"\n",
					"        }\n",
					"    ent_imports.append(d)\n",
					"\n",
					"    ent_definitions = []\n",
					"    d = {\n",
					"      \"entityName\": filename,\n",
					"      \"attributeContext\": {\n",
					"        \"type\": \"entity\",\n",
					"        \"name\": filename,\n",
					"        \"definition\": \"resolvedFrom/\" + filename,\n",
					"        \"contents\": contents\n",
					"      },\n",
					"      \"hasAttributes\": attributes,\n",
					"      \"version\": \"1.0.0.0\"\n",
					"    }\n",
					"    ent_definitions.append(d)\n",
					"    entity_model = {\"jsonSchemaSemanticVersion\": \"1.1.0\",\"imports\":ent_imports, \"definitions\":ent_definitions}\n",
					"    #print(entity_model)\n",
					"\n",
					"    \n",
					"    json_model = json.dumps(entity_model) \n",
					"\n",
					"    # convert to a dataframe\n",
					"    json_list = []\n",
					"    json_list.append(json_model)\n",
					"    df = spark.read.json(sc.parallelize(json_list))\n",
					"    #display(df)\n",
					"\n",
					"    entfilebasepath = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/c360data/' \n",
					"    entjsonpath = entfilebasepath + 'tempfolder'\n",
					"\n",
					"    df.coalesce(1).write.format('json').mode('overwrite').save(entjsonpath)\n",
					"    \n",
					"    # copy the model json file written in parts to a single default.manifest.cdm.json file\n",
					"    from notebookutils import mssparkutils\n",
					"    files = mssparkutils.fs.ls('/c360data/tempfolder')\n",
					"    for file1 in files:\n",
					"        if '.json' in file1.name:\n",
					"            srcfilename = '/c360data/tempfolder/' + file1.name\n",
					"            targetfilename = '/c360data/resolve/' + filename + '.cdm.json'\n",
					"            mssparkutils.fs.cp(srcfilename,targetfilename, True)\n",
					"            break\n",
					"    #delete the folder with parts file        \n",
					"    mssparkutils.fs.rm('/c360data/tempfolder',recurse=True)\n",
					"    #break\n",
					"\n",
					"\n",
					"#create the main manifest.json file\n",
					"imports = []\n",
					"entities = []\n",
					"\n",
					"for filename in filenames:\n",
					"    d = {\n",
					"      \"type\": \"LocalEntity\",\n",
					"      \"entityName\": filename,\n",
					"      \"entityPath\": \"resolve/\" + filename + '.cdm.json/' + filename,\n",
					"      \"dataPartitionPatterns\": [\n",
					"        {\n",
					"          \"name\": filename,\n",
					"          \"rootLocation\": 'workspaces/' + synapse_workspace_name + '/warehouse/c360_data.db/' + filename,\n",
					"          \"regularExpression\": \".+\\\\.parquet$\",\n",
					"          \"parameters\": [],\n",
					"          \"exhibitsTraits\": [\n",
					"            {\"traitReference\" : \"is.partition.format.parquet\"}\n",
					"          ]\n",
					"        }\n",
					"      ],\n",
					"      \"definitions\": get_ci_datedefinitions()\n",
					"    }\n",
					"    entities.append(d)\n",
					"\n",
					"manifest = {\"manifestName\": \"default\",\"entities\": entities,\"jsonSchemaSemanticVersion\": \"1.1.0\",\"imports\":imports}\n",
					"\n",
					"json_model = json.dumps(manifest) \n",
					"\n",
					"# convert to a dataframe\n",
					"json_list = []\n",
					"json_list.append(json_model)\n",
					"df = spark.read.json(sc.parallelize(json_list))\n",
					"#display(df)\n",
					"\n",
					"entfilebasepath = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/c360data/' \n",
					"entjsonpath = entfilebasepath + 'tempfolder'\n",
					"\n",
					"df.coalesce(1).write.format('json').mode('overwrite').save(entjsonpath)\n",
					"\n",
					"# copy the model json file written in parts to a single model.json file\n",
					"from notebookutils import mssparkutils\n",
					"files = mssparkutils.fs.ls('/c360data/tempfolder')\n",
					"for file1 in files:\n",
					"    if '.json' in file1.name:\n",
					"        srcfilename = '/c360data/tempfolder/' + file1.name\n",
					"        targetfilename = '/c360data/' + 'default.manifest.cdm.json'\n",
					"        mssparkutils.fs.cp(srcfilename,targetfilename, True)\n",
					"        break\n",
					"#delete the folder with parts file        \n",
					"mssparkutils.fs.rm('/c360data/tempfolder',recurse=True)\n",
					""
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"try:\n",
					"    mssparkutils.fs.rm('/synapse/default.manifest.cdm.json')\n",
					"    mssparkutils.fs.rm('/synapse/resolve',recurse=True)\n",
					"except:\n",
					"    pass\n",
					"    \n",
					"srcfilename = '/c360data/default.manifest.cdm.json'\n",
					"targetfilename = '/synapse/default.manifest.cdm.json'\n",
					"mssparkutils.fs.cp(srcfilename,targetfilename, True)\n",
					"\n",
					"srcfilename = '/c360data/resolve'\n",
					"targetfilename = '/synapse/resolve'\n",
					"mssparkutils.fs.cp(srcfilename,targetfilename, True)\n",
					"\n",
					"mssparkutils.fs.rm('/c360data',recurse=True)"
				],
				"execution_count": 7
			}
		]
	}
}